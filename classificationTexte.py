import pandas as pd
import string

#for machine learning - classification
from sklearn.svm import LinearSVC
from sklearn.metrics import confusion_matrix, f1_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

#for visualization
import seaborn as sns
import matplotlib.pyplot as plt

#for NLP
from nltk.stem import WordNetLemmatizer
from nltk import word_tokenize
from nltk.corpus import stopwords
import nltk
#nltk.download('stopwords')
#nltk.download('wordnet')
from nltk import ngrams

#les Data
df1 = pd.read_csv("/home/houmenou/Documents/LaPlateforme_Jobs/RoueDesEmotions/YouTube-Spam-Collection-v1/Youtube01-Psy.csv")
df2 = pd.read_csv("/home/houmenou/Documents/LaPlateforme_Jobs/RoueDesEmotions/YouTube-Spam-Collection-v1/Youtube02-KatyPerry.csv")
df3 = pd.read_csv("/home/houmenou/Documents/LaPlateforme_Jobs/RoueDesEmotions/YouTube-Spam-Collection-v1/Youtube03-LMFAO.csv")
df4 = pd.read_csv("/home/houmenou/Documents/LaPlateforme_Jobs/RoueDesEmotions/YouTube-Spam-Collection-v1/Youtube04-Eminem.csv")
df5 = pd.read_csv("/home/houmenou/Documents/LaPlateforme_Jobs/RoueDesEmotions/YouTube-Spam-Collection-v1/Youtube05-Shakira.csv")

df = pd.concat([df1,df2,df3,df4,df5])
df = df[['CONTENT','CLASS']].copy()

#exploration de data
print(df.CLASS.value_counts())
print("#############################################")
print(df.sample(5))

#Preprocessing des donn√©es
print("#############################################")

## Normalization:
#Normaliser le texte signifie le mettre √† la m√™me casse, souvent tout en minuscule.

phrase = "j'ai BEAUCOUP mang√©"
print(phrase.lower())

print("#############################################")

##Tok√©nization

"""Passons √† la Tok√©nization ! C‚Äôest un proc√©d√© tr√®s simple qui divise une cha√Æne de caract√®re en tokens, 
c‚Äôest-√†-dire des √©l√©ments atomiques de la cha√Æne. Un token n‚Äôest pas forc√©ment un mot, ce peut √™tre par 
exemple un signe de ponctuation. NLTK fournit plusieurs types de tok√©nization, comme la tok√©nization par 
mot ou par phrase. En effet, si on consid√®re que la fin d‚Äôune phrase correspond √† un point puis un espace 
puis une majuscule, nous aurions du mal avec les acronymes ou les titres (Dr. Intel). Ce que NLTK propose, 
ce sont donc des tokenizers d√©j√† entra√Æn√©s sur un set de documents (ou corpus). Dans notre cas, nous n‚Äôavons 
besoin que du tokenizer de mots. Exemple:"""

print(word_tokenize('I would like an orangenjuice, and a sandwich!'))
print('###########################################')
##Suppression des stop words

"""Vient ensuite l‚Äô√©tape de suppression des stopwords qui est cruciale, car elle va enlever dans le texte tous 
les mots qui n‚Äôont que peu d‚Äôint√©r√™t s√©mantique. Les stopwords sont en effet tous les mots les plus courants 
d‚Äôune langue (d√©terminants, pronoms, etc..). NLTK dispose d‚Äôune liste de stopwords en anglais (ou dans d‚Äôautres langues). Exemple:"""

stopW = stopwords.words('english')
print('il y a {} stopwords'.format(len(stopW)))
print('les 10 premiers sont {} \n'.format(stopW[:10]))

exclude = set(string.punctuation)
tokens = word_tokenize('I would like an orange juice, and a sandwich!')
print('input tokens: {}'.format(tokens))
stopW.extend(exclude) #we add the punctuation to the previous stop words list
tokens_without_stopW = [word for word in tokens if word not in stopW]
print('output tokens: {}'.format(tokens_without_stopW))

print('###########################################')

##Stemming et Lemmatization
""" Ces deux m√©thodes sont tr√®s couramment utilis√©es dans le traitement du langage naturel car permettent de 
repr√©senter sous un m√™me mot plusieurs d√©riv√©es du mot. Dans le cas du Stemming, nous allons uniquement garder
le radical du mot (ex : dormir, dortoir et dors deviendront dor). La lemmatization, moins radicale üòâ, va laisser 
au mot un sens s√©mantique mais va √©liminer le genre ou le pluriel par exemple. """

lemma = WordNetLemmatizer()
text = word_tokenize('The girls wanted to play with thier parents')
print([lemma.lemmatize(word) for word in text])

""" Nous pouvons maintenant appliquer en un coup la lemmatization et la normalisation √† notre dataframe. 
Ici, nous appliquons la tok√©nization dans le but de faire la lemmatization, mais nous rejoignons les tokens 
(avec la fonction join) car nous allons ici avoir besoin de cette forme plus tard, tout d√©pend de l‚Äôapplication."""

def lemmatizer(sent):
    tokens = word_tokenize(sent.lower())
    tokens = [lemma.lemmatize(lemma.lemmatize(lemma.lemmatize(w,'v'), 'n'), 'a') for w in tokens]
    return ' '.join(tokens)

df['CONTENT'] = df.CONTENT.apply(lambda sent: lemmatizer(sent))
#print(df['CONTENT'])

""" Dans la pratique, le Stemming est employ√© surtout pour effectuer des recherches sur un grand nombre de document (ex : moteur de recherche), pour le reste, la lemmatization est souvent pr√©f√©r√©e.

Ces m√©thodes sont employ√©es pour deux raisons :

-Donner le m√™me sens √† des mots tr√®s proches mais d‚Äôun genre diff√©rent (ou √©liminer le pluriel, etc‚Ä¶)
-R√©duire la sparsit√© des matrices utilis√©es dans les algorithmes (voir partie suivante sur TFIDF)"""

print('###########################################')

## N-grams

""" Les n-grams sont tous simplement des suites de mots pr√©sents dans le texte. 
Ce que nous traitions jusqu‚Äô√† pr√©sent √©taient uniquement des unigrammes, nous 
pouvons ensuite rajouter des bigrammes ou m√™me des trigrammes. Un bigramme est un
 couple de mot qui se suivent dans le texte, nous pouvons les trouver facilement 
 gr√¢ce √† NLTK :"""

tokens = word_tokenize('The girls wanted to play with their pareents')
bigrams = ngrams(tokens,2)
for words in bigrams:
    print(words)

#Classification

"""Vous le savez peut-√™tre mais les algorithmes n‚Äôaiment pas les mots‚Ä¶ Heureusement pour nous, 
il existe des m√©thodes simples permettant de convertir un document en une matrice de mot. 
Ces matrices √©tant souvent creuses (sparse en anglais), c‚Äôest-√†-dire pleines de 0 avec peu de 
valeurs, la lemmatization aide √† r√©duire leurs tailles. Afin de convertir ces phrases en 
matrice, nous allons voir une m√©thode que l‚Äôon appelle TFIDF (Term Frequency ‚Äì Inverse
 Document Frequency). """

## TFIDF
""" TFIDF est une approche bag-of-words (bow) permettant de repr√©senter les mots d‚Äôun document √† l‚Äôaide 
d‚Äôune matrice de nombres. Le terme bow signifie que l‚Äôordre des mots dans la phrase n‚Äôest pas pris en compte,
contrairement √† des approches plus pouss√©es de Deep Learning"""

vect = TfidfVectorizer(stop_words='english',analyzer='word',ngram_range=(1,2))
tfidf_mat = vect.fit_transform(df.CONTENT)
feature_names = vect.get_feature_names() #to get the nams of the tokens
dense = tfidf_mat.todense() #convert sparse matrix to numpy array
denselist = dense.tolist() #convert array to list
df2 = pd.DataFrame(denselist,columns=feature_names) #convert to dataframe
print(df2.head())
print('###########################################')


#Support Vector Machine

""" Une machine √† vecteurs de support (SVM) est un algorithme permettant de r√©aliser des t√¢ches 
de classification ou de r√©gression, tr√®s en vogue il y a quelques ann√©es mais depuis largement 
surpass√© par les r√©seaux de neurones profonds. N√©anmoins, il fonctionne bien sur des donn√©es 
textuelles. Son principe est de s√©parer au maximum les exemples tir√©s des diff√©rentes classes,
 on le qualifie de hard margin classifier """

X_train, X_test, y_train, y_test = train_test_split(df2, df.CLASS, test_size = 0.3)
clf = LinearSVC()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print('F1 score : ', f1_score(y_test, y_pred))

#matrice de confusion concluante
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True)
plt.ylabel('True classes')
plt.xlabel('Predicted classes')
plt.show()

#Pour rappel, une matrice de classification nous renseigne sur le nombre de spam/ham 
#correctement class√©s ou bien sur ceux qui ont √©t√© mal class√©s.

""" Le score F1 est une m√©trique tr√®s utile dans les t√¢ches de classification, nous indiquant 
√† la fois la pr√©cision et le recall du mod√®le, qui se calculent gr√¢ce √† la matrice de confusion
 ci-dessus et dont vous pourrez trouver le d√©tail sur wikip√©dia üòâ"""